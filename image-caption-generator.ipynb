{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":2833644,"sourceType":"datasetVersion","datasetId":1733481}],"dockerImageVersionId":30235,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport json\nimport pandas as pd\nimport re\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport collections\nimport random\nimport requests\nimport json\nfrom math import sqrt\nfrom PIL import Image\nfrom tqdm.auto import tqdm","metadata":{"id":"AldVDvOgcpbc","execution":{"iopub.status.busy":"2024-05-25T21:52:05.653931Z","iopub.execute_input":"2024-05-25T21:52:05.654247Z","iopub.status.idle":"2024-05-25T21:52:10.481730Z","shell.execute_reply.started":"2024-05-25T21:52:05.654182Z","shell.execute_reply":"2024-05-25T21:52:10.480961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '../input/coco-2017-dataset/coco2017'\ninception_v3_mod = '../input/pretrained-models/inception_v3_weights_tf_dim_ordering_tf_kernels_notop (1).h5'","metadata":{"execution":{"iopub.status.busy":"2024-05-25T21:52:10.483297Z","iopub.execute_input":"2024-05-25T21:52:10.483778Z","iopub.status.idle":"2024-05-25T21:52:10.488305Z","shell.execute_reply.started":"2024-05-25T21:52:10.483750Z","shell.execute_reply":"2024-05-25T21:52:10.487018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(f'{BASE_PATH}/annotations/captions_train2017.json', 'r') as f:\n    data = json.load(f)\n    data = data['annotations']\n\nimg_cap_pairs = []\n\nfor sample in data:\n    img_name = '%012d.jpg' % sample['image_id']\n    img_cap_pairs.append([img_name, sample['caption']])\n\ncaptions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\ncaptions['image'] = captions['image'].apply(\n    lambda x: f'{BASE_PATH}/train2017/{x}'\n)\ncaptions = captions.sample(70000)\ncaptions = captions.reset_index(drop=True)\ncaptions.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T21:07:39.047182Z","iopub.execute_input":"2024-05-24T21:07:39.047569Z","iopub.status.idle":"2024-05-24T21:07:42.668102Z","shell.execute_reply.started":"2024-05-24T21:07:39.047533Z","shell.execute_reply":"2024-05-24T21:07:42.666958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    text = '[start] ' + text + ' [end]'\n    return text","metadata":{"id":"rWbe_xuhFaJp","execution":{"iopub.status.busy":"2024-05-24T21:07:42.669381Z","iopub.execute_input":"2024-05-24T21:07:42.669754Z","iopub.status.idle":"2024-05-24T21:07:42.677827Z","shell.execute_reply.started":"2024-05-24T21:07:42.669719Z","shell.execute_reply":"2024-05-24T21:07:42.676505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions['caption'] = captions['caption'].apply(preprocess)\ncaptions.head()","metadata":{"id":"v_ouwWhKnEy5","outputId":"d190c744-d31e-430b-ed85-eb0295010c1d","execution":{"iopub.status.busy":"2024-05-24T21:07:42.680931Z","iopub.execute_input":"2024-05-24T21:07:42.681306Z","iopub.status.idle":"2024-05-24T21:07:43.243637Z","shell.execute_reply.started":"2024-05-24T21:07:42.681271Z","shell.execute_reply":"2024-05-24T21:07:43.242511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_row = captions.sample(1).iloc[0]\nprint(random_row.caption)\nprint()\nim = Image.open(random_row.image)\nim","metadata":{"id":"6RBuExHWnGEt","outputId":"0242452f-4d17-4af6-a9bb-3bea7b09568e","execution":{"iopub.status.busy":"2024-05-24T21:07:43.245072Z","iopub.execute_input":"2024-05-24T21:07:43.245447Z","iopub.status.idle":"2024-05-24T21:07:43.518475Z","shell.execute_reply.started":"2024-05-24T21:07:43.245397Z","shell.execute_reply":"2024-05-24T21:07:43.517185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 40\nVOCABULARY_SIZE = 15000\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nEMBEDDING_DIM = 512\nUNITS = 512\nEPOCHS = 5","metadata":{"id":"nSTivH_FSSf2","execution":{"iopub.status.busy":"2024-05-24T21:07:43.519832Z","iopub.execute_input":"2024-05-24T21:07:43.520169Z","iopub.status.idle":"2024-05-24T21:07:43.525488Z","shell.execute_reply.started":"2024-05-24T21:07:43.520139Z","shell.execute_reply":"2024-05-24T21:07:43.524486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = tf.keras.layers.TextVectorization(\n    max_tokens=VOCABULARY_SIZE,\n    standardize=None,\n    output_sequence_length=MAX_LENGTH)\n\ntokenizer.adapt(captions['caption'])","metadata":{"id":"X8MGUNtBN2sz","execution":{"iopub.status.busy":"2024-05-24T21:07:43.527312Z","iopub.execute_input":"2024-05-24T21:07:43.527666Z","iopub.status.idle":"2024-05-24T21:07:46.819151Z","shell.execute_reply.started":"2024-05-24T21:07:43.527634Z","shell.execute_reply":"2024-05-24T21:07:46.818003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.vocabulary_size()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T21:07:46.820581Z","iopub.execute_input":"2024-05-24T21:07:46.820957Z","iopub.status.idle":"2024-05-24T21:07:46.828286Z","shell.execute_reply.started":"2024-05-24T21:07:46.820923Z","shell.execute_reply":"2024-05-24T21:07:46.827248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\npickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T21:07:46.829821Z","iopub.execute_input":"2024-05-24T21:07:46.830162Z","iopub.status.idle":"2024-05-24T21:07:46.961116Z","shell.execute_reply.started":"2024-05-24T21:07:46.830130Z","shell.execute_reply":"2024-05-24T21:07:46.959921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2idx = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary())\n\nidx2word = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary(),\n    invert=True)","metadata":{"id":"qvhg-6eKN3nz","execution":{"iopub.status.busy":"2024-05-24T21:07:46.962488Z","iopub.execute_input":"2024-05-24T21:07:46.962818Z","iopub.status.idle":"2024-05-24T21:07:47.251025Z","shell.execute_reply.started":"2024-05-24T21:07:46.962789Z","shell.execute_reply":"2024-05-24T21:07:47.249634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(captions['image'], captions['caption']):\n    img_to_cap_vector[img].append(cap)\n\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_keys = (img_keys[:slice_index], \n                                          img_keys[slice_index:])\n\ntrain_imgs = []\ntrain_captions = []\nfor imgt in img_name_train_keys:\n    capt_len = len(img_to_cap_vector[imgt])\n    train_imgs.extend([imgt] * capt_len)\n    train_captions.extend(img_to_cap_vector[imgt])\n\nval_imgs = []\nval_captions = []\nfor imgv in img_name_val_keys:\n    capv_len = len(img_to_cap_vector[imgv])\n    val_imgs.extend([imgv] * capv_len)\n    val_captions.extend(img_to_cap_vector[imgv])","metadata":{"id":"Yrca2aN2N5WL","execution":{"iopub.status.busy":"2024-05-24T21:07:47.252285Z","iopub.execute_input":"2024-05-24T21:07:47.252648Z","iopub.status.idle":"2024-05-24T21:07:47.475088Z","shell.execute_reply.started":"2024-05-24T21:07:47.252615Z","shell.execute_reply":"2024-05-24T21:07:47.474089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)","metadata":{"id":"UHN3Q1YDN5TD","outputId":"0b0af2ea-f6d7-48c9-ba30-14d8d9c98418","execution":{"iopub.status.busy":"2024-05-24T21:07:47.476478Z","iopub.execute_input":"2024-05-24T21:07:47.476933Z","iopub.status.idle":"2024-05-24T21:07:47.485017Z","shell.execute_reply.started":"2024-05-24T21:07:47.476891Z","shell.execute_reply":"2024-05-24T21:07:47.483768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(img_path, caption):\n    img = tf.io.read_file(img_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    caption = tokenizer(caption)\n    return img, caption","metadata":{"id":"12c-7FHzOFSq","execution":{"iopub.status.busy":"2024-05-24T21:07:47.489467Z","iopub.execute_input":"2024-05-24T21:07:47.490256Z","iopub.status.idle":"2024-05-24T21:07:47.496698Z","shell.execute_reply.started":"2024-05-24T21:07:47.490216Z","shell.execute_reply":"2024-05-24T21:07:47.495594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (train_imgs, train_captions))\n\ntrain_dataset = train_dataset.map(\n    load_data, num_parallel_calls=tf.data.AUTOTUNE\n    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    (val_imgs, val_captions))\n\nval_dataset = val_dataset.map(\n    load_data, num_parallel_calls=tf.data.AUTOTUNE\n    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"id":"vHk83y3eOFPz","execution":{"iopub.status.busy":"2024-05-24T21:07:47.497903Z","iopub.execute_input":"2024-05-24T21:07:47.498243Z","iopub.status.idle":"2024-05-24T21:07:48.275171Z","shell.execute_reply.started":"2024-05-24T21:07:47.498213Z","shell.execute_reply":"2024-05-24T21:07:48.273704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_augmentation = tf.keras.Sequential(\n    [\n        tf.keras.layers.RandomFlip(\"horizontal\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomContrast(0.3),\n    ]\n)","metadata":{"id":"bQr_bgk11eMF","execution":{"iopub.status.busy":"2024-05-24T21:07:48.276839Z","iopub.execute_input":"2024-05-24T21:07:48.277243Z","iopub.status.idle":"2024-05-24T21:07:48.305092Z","shell.execute_reply.started":"2024-05-24T21:07:48.277203Z","shell.execute_reply":"2024-05-24T21:07:48.304187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import InceptionV3\ninception_v3 = InceptionV3(\n    include_top=False,\n    weights=None  # We will load the weights manually\n)\ninception_v3.load_weights(inception_v3_mod)\ndef CNN_Encoder():\n#     inception_v3 = tf.keras.applications.InceptionV3(\n#         include_top=False,\n#         weights='imagenet'\n#     )\n\n    output = inception_v3.output\n    output = tf.keras.layers.Reshape(\n        (-1, output.shape[-1]))(output)\n\n    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n    return cnn_model","metadata":{"id":"H9GDJ9_1nIMO","execution":{"iopub.status.busy":"2024-05-25T21:52:57.232201Z","iopub.execute_input":"2024-05-25T21:52:57.233128Z","iopub.status.idle":"2024-05-25T21:53:02.960040Z","shell.execute_reply.started":"2024-05-25T21:52:57.233089Z","shell.execute_reply":"2024-05-25T21:53:02.958686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n        self.attention = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n    \n\n    def call(self, x, training):\n        x = self.layer_norm_1(x)\n        x = self.dense(x)\n\n        attn_output = self.attention(\n            query=x,\n            value=x,\n            key=x,\n            attention_mask=None,\n            training=training\n        )\n\n        x = self.layer_norm_2(x + attn_output)\n        return x","metadata":{"id":"jMy5MrE2PdHV","execution":{"iopub.status.busy":"2024-05-24T21:07:48.314151Z","iopub.execute_input":"2024-05-24T21:07:48.314540Z","iopub.status.idle":"2024-05-24T21:07:48.327985Z","shell.execute_reply.started":"2024-05-24T21:07:48.314498Z","shell.execute_reply":"2024-05-24T21:07:48.326962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Embeddings(tf.keras.layers.Layer):\n\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embeddings = tf.keras.layers.Embedding(\n            vocab_size, embed_dim)\n        self.position_embeddings = tf.keras.layers.Embedding(\n            max_len, embed_dim, input_shape=(None, max_len))\n    \n\n    def call(self, input_ids):\n        length = tf.shape(input_ids)[-1]\n        position_ids = tf.range(start=0, limit=length, delta=1)\n        position_ids = tf.expand_dims(position_ids, axis=0)\n\n        token_embeddings = self.token_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n\n        return token_embeddings + position_embeddings","metadata":{"id":"MFqNFts0duGB","execution":{"iopub.status.busy":"2024-05-24T21:07:48.329258Z","iopub.execute_input":"2024-05-24T21:07:48.329639Z","iopub.status.idle":"2024-05-24T21:07:48.339501Z","shell.execute_reply.started":"2024-05-24T21:07:48.329605Z","shell.execute_reply":"2024-05-24T21:07:48.338489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self, embed_dim, units, num_heads):\n        super().__init__()\n        self.embedding = Embeddings(\n            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n\n        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n\n        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n\n        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n\n        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n\n        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n    \n\n    def call(self, input_ids, encoder_output, training, mask=None):\n        embeddings = self.embedding(input_ids)\n\n        combined_mask = None\n        padding_mask = None\n        \n        if mask is not None:\n            causal_mask = self.get_causal_attention_mask(embeddings)\n            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n            combined_mask = tf.minimum(combined_mask, causal_mask)\n\n        attn_output_1 = self.attention_1(\n            query=embeddings,\n            value=embeddings,\n            key=embeddings,\n            attention_mask=combined_mask,\n            training=training\n        )\n\n        out_1 = self.layernorm_1(embeddings + attn_output_1)\n\n        attn_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_output,\n            key=encoder_output,\n            attention_mask=padding_mask,\n            training=training\n        )\n\n        out_2 = self.layernorm_2(out_1 + attn_output_2)\n\n        ffn_out = self.ffn_layer_1(out_2)\n        ffn_out = self.dropout_1(ffn_out, training=training)\n        ffn_out = self.ffn_layer_2(ffn_out)\n\n        ffn_out = self.layernorm_3(ffn_out + out_2)\n        ffn_out = self.dropout_2(ffn_out, training=training)\n        preds = self.out(ffn_out)\n        return preds\n\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0\n        )\n        return tf.tile(mask, mult)","metadata":{"id":"pcbCQqrDnJ4-","execution":{"iopub.status.busy":"2024-05-24T21:07:48.340826Z","iopub.execute_input":"2024-05-24T21:07:48.341206Z","iopub.status.idle":"2024-05-24T21:07:48.360022Z","shell.execute_reply.started":"2024-05-24T21:07:48.341172Z","shell.execute_reply":"2024-05-24T21:07:48.358820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageCaptioningModel(tf.keras.Model):\n\n    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n        super().__init__()\n        self.cnn_model = cnn_model\n        self.encoder = encoder\n        self.decoder = decoder\n        self.image_aug = image_aug\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n\n\n    def calculate_loss(self, y_true, y_pred, mask):\n        loss = self.loss(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n\n    def calculate_accuracy(self, y_true, y_pred, mask):\n        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n        accuracy = tf.math.logical_and(mask, accuracy)\n        accuracy = tf.cast(accuracy, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n    \n\n    def compute_loss_and_acc(self, img_embed, captions, training=True):\n        encoder_output = self.encoder(img_embed, training=True)\n        y_input = captions[:, :-1]\n        y_true = captions[:, 1:]\n        mask = (y_true != 0)\n        y_pred = self.decoder(\n            y_input, encoder_output, training=True, mask=mask\n        )\n        loss = self.calculate_loss(y_true, y_pred, mask)\n        acc = self.calculate_accuracy(y_true, y_pred, mask)\n        return loss, acc\n\n    \n    def train_step(self, batch):\n        imgs, captions = batch\n\n        if self.image_aug:\n            imgs = self.image_aug(imgs)\n        \n        img_embed = self.cnn_model(imgs)\n\n        with tf.GradientTape() as tape:\n            loss, acc = self.compute_loss_and_acc(\n                img_embed, captions\n            )\n    \n        train_vars = (\n            self.encoder.trainable_variables + self.decoder.trainable_variables\n        )\n        grads = tape.gradient(loss, train_vars)\n        self.optimizer.apply_gradients(zip(grads, train_vars))\n        self.loss_tracker.update_state(loss)\n        self.acc_tracker.update_state(acc)\n\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n    \n\n    def test_step(self, batch):\n        imgs, captions = batch\n\n        img_embed = self.cnn_model(imgs)\n\n        loss, acc = self.compute_loss_and_acc(\n            img_embed, captions, training=False\n        )\n\n        self.loss_tracker.update_state(loss)\n        self.acc_tracker.update_state(acc)\n\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.acc_tracker]","metadata":{"id":"9_NmSUaVys9R","execution":{"iopub.status.busy":"2024-05-24T21:07:48.361661Z","iopub.execute_input":"2024-05-24T21:07:48.362145Z","iopub.status.idle":"2024-05-24T21:07:48.381545Z","shell.execute_reply.started":"2024-05-24T21:07:48.362103Z","shell.execute_reply":"2024-05-24T21:07:48.380612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\ndecoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n\ncnn_model = CNN_Encoder()\ncaption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)","metadata":{"id":"GqWpcsje0Hkh","outputId":"477f4a81-1e19-445a-d64d-cedad90a2893","execution":{"iopub.status.busy":"2024-05-24T21:07:48.382827Z","iopub.execute_input":"2024-05-24T21:07:48.383153Z","iopub.status.idle":"2024-05-24T21:08:10.616154Z","shell.execute_reply.started":"2024-05-24T21:07:48.383122Z","shell.execute_reply":"2024-05-24T21:08:10.614463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=\"none\"\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\ncaption_model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=cross_entropy\n)","metadata":{"id":"bayNssgNX6QN","execution":{"iopub.status.busy":"2024-05-24T21:08:10.617850Z","iopub.status.idle":"2024-05-24T21:08:10.618315Z","shell.execute_reply.started":"2024-05-24T21:08:10.618098Z","shell.execute_reply":"2024-05-24T21:08:10.618120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = caption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=[early_stopping]\n)","metadata":{"id":"1RYo-MRVYn49","execution":{"iopub.status.busy":"2024-05-24T21:08:10.619605Z","iopub.status.idle":"2024-05-24T21:08:10.620097Z","shell.execute_reply.started":"2024-05-24T21:08:10.619862Z","shell.execute_reply":"2024-05-24T21:08:10.619882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='validation loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T21:08:10.621404Z","iopub.status.idle":"2024-05-24T21:08:10.621830Z","shell.execute_reply.started":"2024-05-24T21:08:10.621634Z","shell.execute_reply":"2024-05-24T21:08:10.621653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_from_path(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img\n\n\ndef generate_caption(img_path, add_noise=False):\n    img = load_image_from_path(img_path)\n    \n    if add_noise:\n        noise = tf.random.normal(img.shape)*0.1\n        img = img + noise\n        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n    \n    img = tf.expand_dims(img, axis=0)\n    img_embed = caption_model.cnn_model(img)\n    img_encoded = caption_model.encoder(img_embed, training=False)\n\n    y_inp = '[start]'\n    for i in range(MAX_LENGTH-1):\n        tokenized = tokenizer([y_inp])[:, :-1]\n        mask = tf.cast(tokenized != 0, tf.int32)\n        pred = caption_model.decoder(\n            tokenized, img_encoded, training=False, mask=mask)\n        \n        pred_idx = np.argmax(pred[0, i, :])\n        pred_idx = tf.convert_to_tensor(pred_idx)\n        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n        if pred_word == '[end]':\n            break\n        \n        y_inp += ' ' + pred_word\n    \n    y_inp = y_inp.replace('[start] ', '')\n    return y_inp","metadata":{"id":"3ErlQQICtj_g","execution":{"iopub.status.busy":"2024-05-24T21:08:10.624373Z","iopub.status.idle":"2024-05-24T21:08:10.624960Z","shell.execute_reply.started":"2024-05-24T21:08:10.624667Z","shell.execute_reply":"2024-05-24T21:08:10.624695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = random.randrange(0, len(captions))\nimg_path = captions.iloc[idx].image\n\npred_caption = generate_caption(img_path)\nprint('Predicted Caption:', pred_caption)\nprint()\nImage.open(img_path)","metadata":{"id":"27_bJe_M1Drr","execution":{"iopub.status.busy":"2024-05-24T21:08:10.626601Z","iopub.status.idle":"2024-05-24T21:08:10.627153Z","shell.execute_reply.started":"2024-05-24T21:08:10.626868Z","shell.execute_reply":"2024-05-24T21:08:10.626895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_url = \"https://images.squarespace-cdn.com/content/v1/5e0e65adcd39ed279a0402fd/1627422658456-7QKPXTNQ34W2OMBTESCJ/1.jpg?format=2500w\"\n\n# im = Image.open(requests.get(img_url, stream=True).raw)\n# im = im.convert('RGB')\n# im.save('tmp.jpg')\n\n# pred_caption = generate_caption('tmp.jpg', add_noise=False)\n# print('Predicted Caption:', pred_caption)\n# print()\n# im","metadata":{"execution":{"iopub.status.busy":"2024-05-24T21:08:10.629797Z","iopub.status.idle":"2024-05-24T21:08:10.630363Z","shell.execute_reply.started":"2024-05-24T21:08:10.630067Z","shell.execute_reply":"2024-05-24T21:08:10.630093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model.save_weights('model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-05-24T21:08:10.631484Z","iopub.status.idle":"2024-05-24T21:08:10.631926Z","shell.execute_reply.started":"2024-05-24T21:08:10.631723Z","shell.execute_reply":"2024-05-24T21:08:10.631744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"XG69m29gs6W4"},"execution_count":null,"outputs":[]}]}